Title: Zen and the Art of Machine Translation: A Hands-on Workshop for Open-Source Taoist Text Processing

Abstract:
Machine Translation (MT) has advanced significantly with the rise of Large Language Models (LLMs), yet many solutions remain dependent on proprietary systems, restricting accessibility and customization. This session will demonstrate how to construct a robust, open-source MT pipeline tailored for domain-specific content. Using a dataset composed of 75% Chinese and 25% Japanese texts from Taoist and Zen Buddhist traditions, we will explore best practices for fine-tuning models to handle specialized corpora.

We will begin with a historical overview of MT from the 1940s to the present, highlighting key developments such as Statistical Machine Translation (SMT) and vectorization techniques. Then, we will transition to modern methodologies, comparing various open-source models available on Hugging Face, such as mBERT, Deepseek, and NLLB-200. Finally, we will demonstrate how to fine-tune these models for domain-specific translation tasks using traditional Taoist texts.

The session will emphasize practical MT techniques, ensuring attendees gain actionable insights into translation pipeline construction. We will address challenges related to working with unbalanced datasets, focusing on preprocessing, model training, and evaluation. A brief segment will introduce a lightweight retrieval mechanism and chatbot integration, enhancing the practical deployment of translated content.

Pacing Strategy & Deliverables:
To ensure efficient use of time, we will follow a structured approach:

10 min: Introduction – Historical context, overview of SMT and vectorization techniques.

20 min: Fine-Tuning Transformers – Basics & Hugging Face Walkthrough.

15 min: Comparing Open-Source MT Models – Performance & Tradeoffs.

60 min: Hands-On – Building the MT Pipeline for Classical Taoist and Zen Buddhist Texts.

Part 1 (20 min): Data Preparation & Model Setup.

Part 2 (30 min): Fine-Tuning & Evaluation.

Part 3 (10 min): Lightweight retrieval and chatbot integration.

15 min: Discussion and Q&A on Practical Deployment.

Attendees will leave with:

A reproducible Jupyter Notebook containing code for MT pipeline construction.

Access to a curated multilingual dataset of Taoist texts (75% Chinese, 25% Japanese).

A structured framework for fine-tuning MT models for domain-specific applications.

Key Takeaways:

Understand the fundamentals of MT and its evolution.

Learn how to integrate open-source models like NLLB-200 with vector databases.

Explore domain adaptation strategies to enhance translation accuracy.

Compare different open-source MT models available on Hugging Face (e.g., mBERT, NLLB-200).

Gain hands-on insights into evaluating MT performance in specialized domains.

See a real-world implementation of Taoist and Zen Buddhist text translation using open-source tools.

Target Audience:
Introductory-Intermediate. Ideal for ML engineers, data scientists, and AI practitioners interested in machine translation, LLMs, and open-source AI development. A basic understanding of NLP concepts, such as tokenization, embeddings, and transformer-based models, will be helpful but not required.

Session Format:
2-hour talk with a live demo showcasing an open-source MT pipeline in action.

Speaker Commitment:
I understand the requirements of ODSC and will ensure the proposal adheres to the non-promotional guidelines. I am prepared to actively promote the session on social media and will provide all required presentation materials within the designated timelines
